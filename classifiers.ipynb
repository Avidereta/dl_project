{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import cPickle\n",
    "import itertools\n",
    "\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 3.15 s, total: 18.5 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('data_basic.pkl', 'rb') as f:\n",
    "    _, _, y_train, _, _, y_val, _, _, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 6.16 s, total: 28.7 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('data_rem_road.pkl', 'rb') as f:\n",
    "    _, _, y_train, _, _, y_val, _, _, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('data_huge.pkl', 'rb') as f:\n",
    "    _, _, y_train, _, _, y_val, _, _, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 22.7 s, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('data_huge_rem_road.pkl', 'rb') as f:\n",
    "    _, _, y_train, _, _, y_val, _, _, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2975, (18432,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f1_train), f1_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 1.18 s, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('features_rem_road.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 27s, sys: 3.83 s, total: 8min 31s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open('features_huge_rem_road.pkl', 'rb') as f:\n",
    "    f1_train, f2_train = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('features_basic.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('features_huge.pkl', 'rb') as f:\n",
    "    f1_train, f2_train = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8c71f5034b91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(y_train, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2975, array([86, 93, 43, ..., 39, 97, 95], dtype=uint8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [np.percentile(y_train, 0),\n",
    "        np.percentile(y_train, 25),\n",
    "        np.percentile(y_train, 50),\n",
    "        np.percentile(y_train, 75),\n",
    "        np.percentile(y_train, 100) + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.0, 57.0, 89.0, 103.0, 127.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = np.digitize(y_train, bins) - 1\n",
    "y_val = np.digitize(y_val, bins) - 1\n",
    "y_test = np.digitize(y_test, bins) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 739.,    0.,    0.,  715.,    0.,    0.,  721.,    0.,    0.,  800.]),\n",
       " array([ 0. ,  0.3,  0.6,  0.9,  1.2,  1.5,  1.8,  2.1,  2.4,  2.7,  3. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6f3844d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68000000000000005"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1')\n",
    "clf.fit(f1_train, y_train).score(f1_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67800000000000005"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty='l1')\n",
    "clf.fit(np.hstack((f1_train, f2_train)), y_train).score(np.hstack((f1_val, f2_val)), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18432,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer().fit(f1_train)\n",
    "f1_train_normalized = normalizer.transform(f1_train)\n",
    "f1_val_normalized = normalizer.transform(f1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=300).fit(f1_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_train_pca = pca.transform(f1_train_normalized)\n",
    "f1_val_pca = pca.transform(f1_val_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59799999999999998"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GradientBoostingClassifier(n_estimators=200).fit(f1_train_pca, y_train).score(f1_val_pca, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the road + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67600000000000005"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.005, penalty='l1')\n",
    "clf.fit(np.hstack((np.array(f1_train), np.array(f2_train))), y_train).score(np.hstack((np.array(f1_val), np.array(f2_val))), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Road deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68000000000000005"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1')\n",
    "clf.fit(np.array(f1_train), y_train).score(np.array(f1_val), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(np.array(f1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137,  10,   0,   2],\n",
       "       [ 29,  78,  15,   6],\n",
       "       [  8,  13,  40,  32],\n",
       "       [  9,   9,  27,  85]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 3, 3, 0, 3,\n",
       "       2, 3, 0, 3, 0, 2, 2, 3, 1, 1, 3, 2, 0, 0, 3, 3, 2, 0, 0, 0, 1, 1, 0,\n",
       "       3, 0, 0, 1, 1, 0, 1, 3, 0, 0, 0, 0, 0, 3, 0, 2, 3, 3, 1, 0, 3, 0, 0,\n",
       "       0, 0, 0, 0, 0, 3, 0, 1, 1, 3, 3, 3, 1, 1, 3, 2, 3, 3, 0, 3, 1, 3, 3,\n",
       "       3, 0, 0, 3, 2, 3, 2, 1, 3, 1, 3, 2, 0, 0, 0, 3, 3, 0, 2, 0, 3, 3, 0,\n",
       "       1, 1, 1, 3, 0, 3, 0, 0, 0, 3, 3, 3, 0, 3, 0, 0, 1, 0, 0, 0, 1, 1, 3,\n",
       "       3, 0, 3, 0, 0, 1, 3, 0, 3, 0, 3, 3, 0, 3, 3, 1, 3, 0, 0, 3, 3, 0, 0,\n",
       "       0, 0, 3, 0, 1, 1, 1, 0, 3, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0,\n",
       "       0, 2, 0, 1, 0, 0, 0, 3, 1, 3, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 1,\n",
       "       1, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 1, 1, 1, 0, 1, 3, 0,\n",
       "       0, 0, 3, 1, 0, 0, 1, 0, 3, 3, 3, 0, 3, 3, 0, 3, 3, 0, 0, 1, 0, 0, 3,\n",
       "       1, 0, 1, 2, 2, 1, 3, 1, 3, 3, 3, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 2, 1,\n",
       "       1, 1, 2, 2, 2, 2, 0, 0, 2, 3, 3, 2, 2, 3, 2, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       2, 0, 2, 3, 0, 1, 1, 0, 3, 0, 1, 0, 3, 3, 2, 3, 3, 2, 3, 3, 2, 1, 3,\n",
       "       2, 2, 1, 1, 1, 0, 2, 0, 0, 1, 2, 2, 3, 3, 1, 1, 1, 0, 0, 1, 2, 3, 2,\n",
       "       2, 1, 1, 0, 2, 0, 0, 3, 1, 0, 2, 1, 3, 0, 0, 1, 3, 3, 0, 0, 0, 3, 0,\n",
       "       2, 3, 0, 1, 2, 2, 2, 0, 0, 0, 2, 2, 1, 1, 3, 2, 2, 3, 2, 1, 2, 2, 2,\n",
       "       1, 0, 2, 0, 1, 3, 0, 1, 1, 2, 1, 2, 0, 1, 2, 2, 3, 3, 1, 2, 1, 2, 1,\n",
       "       2, 0, 0, 0, 2, 1, 0, 3, 2, 1, 3, 3, 0, 1, 0, 0, 1, 0, 2, 2, 1, 0, 2,\n",
       "       1, 2, 0, 3, 0, 3, 1, 0, 2, 1, 0, 2, 1, 1, 3, 0, 3, 0, 0, 2, 3, 1, 3,\n",
       "       0, 0, 0, 1, 0, 2, 1, 1, 2, 3, 1, 0, 0, 1, 3, 2, 3, 1, 3, 1, 2, 0, 0,\n",
       "       1, 1, 2, 2, 3, 0, 3, 2, 1, 0, 2, 0, 0, 0, 1, 3, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = np.array(f1_train).shape[1]\n",
    "k = n / 3\n",
    "P = np.random.randn(n, k) / np.sqrt(k)\n",
    "\n",
    "f1_train_p = np.array(f1_train).dot(P)\n",
    "f1_val_p = np.array(f1_val).dot(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66000000000000003"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1')\n",
    "clf.fit(f1_train_p, y_train).score(f1_val_p, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer().fit(f1_train)\n",
    "f1_train_normalized = normalizer.transform(f1_train)\n",
    "f1_val_normalized = normalizer.transform(f1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000000000000004"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=15, penalty='l2')\n",
    "clf.fit(f1_train_normalized, y_train).score(f1_val_normalized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000000000000004"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stacking import Stacking, Classifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def logit_proba_fitter(X, y):\n",
    "    classifier = LogisticRegression(C=15, penalty='l2').fit(X, y)\n",
    "    return Classifier(classifier.predict_proba)\n",
    "\n",
    "def logit_fitter(X, y):\n",
    "    classifier = LinearSVC(C=0.5).fit(X, y)\n",
    "    return Classifier(classifier.predict)\n",
    "\n",
    "logit_logit = Stacking(base_fitter=logit_proba_fitter, meta_fitter=logit_fitter,\n",
    "                       split=lambda I: list(KFold(n=I.size, n_folds=10, shuffle=True)))\n",
    "logit_logit.fit(f1_train_normalized, y_train).score(f1_val_normalized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67600000000000005"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(C=0.5)\n",
    "clf.fit(f1_train_normalized, y_train).score(f1_val_normalized, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road deleted + segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66600000000000004"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty='l1')\n",
    "clf.fit(np.hstack((np.array(f1_train), np.array(f2_train))), y_train).score(np.hstack((np.array(f1_val), np.array(f2_val))), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network deleted road, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b, W, b, W, b]\n",
      "Epoch 1 of 30 took 1.998s\n",
      "  training loss (in-iteration):\t\t3749.619600\n",
      "  train accuracy:\t\t28.59 %\n",
      "  validation accuracy:\t\t26.00 %\n",
      "Epoch 2 of 30 took 2.015s\n",
      "  training loss (in-iteration):\t\t3.113019\n",
      "  train accuracy:\t\t32.24 %\n",
      "  validation accuracy:\t\t46.60 %\n",
      "Epoch 3 of 30 took 1.982s\n",
      "  training loss (in-iteration):\t\t2.176733\n",
      "  train accuracy:\t\t45.90 %\n",
      "  validation accuracy:\t\t49.40 %\n",
      "Epoch 4 of 30 took 2.010s\n",
      "  training loss (in-iteration):\t\t1.896570\n",
      "  train accuracy:\t\t52.72 %\n",
      "  validation accuracy:\t\t58.00 %\n",
      "Epoch 5 of 30 took 1.978s\n",
      "  training loss (in-iteration):\t\t1.765641\n",
      "  train accuracy:\t\t58.38 %\n",
      "  validation accuracy:\t\t52.20 %\n",
      "Epoch 6 of 30 took 2.000s\n",
      "  training loss (in-iteration):\t\t1.671071\n",
      "  train accuracy:\t\t63.17 %\n",
      "  validation accuracy:\t\t62.00 %\n",
      "Epoch 7 of 30 took 2.000s\n",
      "  training loss (in-iteration):\t\t1.551463\n",
      "  train accuracy:\t\t69.31 %\n",
      "  validation accuracy:\t\t55.20 %\n",
      "Epoch 8 of 30 took 2.111s\n",
      "  training loss (in-iteration):\t\t1.477413\n",
      "  train accuracy:\t\t71.52 %\n",
      "  validation accuracy:\t\t66.60 %\n",
      "Epoch 9 of 30 took 2.005s\n",
      "  training loss (in-iteration):\t\t1.404215\n",
      "  train accuracy:\t\t76.62 %\n",
      "  validation accuracy:\t\t59.40 %\n",
      "Epoch 10 of 30 took 1.980s\n",
      "  training loss (in-iteration):\t\t1.373313\n",
      "  train accuracy:\t\t76.55 %\n",
      "  validation accuracy:\t\t67.40 %\n",
      "Epoch 11 of 30 took 1.987s\n",
      "  training loss (in-iteration):\t\t1.343516\n",
      "  train accuracy:\t\t77.24 %\n",
      "  validation accuracy:\t\t60.80 %\n",
      "Epoch 12 of 30 took 2.008s\n",
      "  training loss (in-iteration):\t\t1.256678\n",
      "  train accuracy:\t\t81.34 %\n",
      "  validation accuracy:\t\t61.20 %\n",
      "Epoch 13 of 30 took 2.102s\n",
      "  training loss (in-iteration):\t\t1.214769\n",
      "  train accuracy:\t\t83.93 %\n",
      "  validation accuracy:\t\t66.20 %\n",
      "Epoch 14 of 30 took 1.999s\n",
      "  training loss (in-iteration):\t\t1.203365\n",
      "  train accuracy:\t\t84.66 %\n",
      "  validation accuracy:\t\t61.60 %\n",
      "Epoch 15 of 30 took 1.974s\n",
      "  training loss (in-iteration):\t\t1.129455\n",
      "  train accuracy:\t\t87.41 %\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 16 of 30 took 2.126s\n",
      "  training loss (in-iteration):\t\t1.263559\n",
      "  train accuracy:\t\t85.00 %\n",
      "  validation accuracy:\t\t67.40 %\n",
      "Epoch 17 of 30 took 1.980s\n",
      "  training loss (in-iteration):\t\t1.062010\n",
      "  train accuracy:\t\t91.52 %\n",
      "  validation accuracy:\t\t65.40 %\n",
      "Epoch 18 of 30 took 1.974s\n",
      "  training loss (in-iteration):\t\t1.000695\n",
      "  train accuracy:\t\t93.72 %\n",
      "  validation accuracy:\t\t64.40 %\n",
      "Epoch 19 of 30 took 2.005s\n",
      "  training loss (in-iteration):\t\t0.974304\n",
      "  train accuracy:\t\t94.59 %\n",
      "  validation accuracy:\t\t64.00 %\n",
      "Epoch 20 of 30 took 1.980s\n",
      "  training loss (in-iteration):\t\t0.925185\n",
      "  train accuracy:\t\t96.62 %\n",
      "  validation accuracy:\t\t62.00 %\n",
      "Epoch 21 of 30 took 2.003s\n",
      "  training loss (in-iteration):\t\t0.937583\n",
      "  train accuracy:\t\t96.28 %\n",
      "  validation accuracy:\t\t62.60 %\n",
      "Epoch 22 of 30 took 1.997s\n",
      "  training loss (in-iteration):\t\t1.025416\n",
      "  train accuracy:\t\t93.21 %\n",
      "  validation accuracy:\t\t62.00 %\n",
      "Epoch 23 of 30 took 1.987s\n",
      "  training loss (in-iteration):\t\t1.056850\n",
      "  train accuracy:\t\t92.55 %\n",
      "  validation accuracy:\t\t63.80 %\n",
      "Epoch 24 of 30 took 1.997s\n",
      "  training loss (in-iteration):\t\t0.880320\n",
      "  train accuracy:\t\t98.07 %\n",
      "  validation accuracy:\t\t64.40 %\n",
      "Epoch 25 of 30 took 2.182s\n",
      "  training loss (in-iteration):\t\t0.861131\n",
      "  train accuracy:\t\t98.76 %\n",
      "  validation accuracy:\t\t64.80 %\n",
      "Epoch 26 of 30 took 2.169s\n",
      "  training loss (in-iteration):\t\t0.866686\n",
      "  train accuracy:\t\t98.07 %\n",
      "  validation accuracy:\t\t64.60 %\n",
      "Epoch 27 of 30 took 2.097s\n",
      "  training loss (in-iteration):\t\t0.839886\n",
      "  train accuracy:\t\t99.03 %\n",
      "  validation accuracy:\t\t64.80 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d82e6a4058e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[0;32m    814\u001b[0m                             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[1;31m# data has to be converted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                     \u001b[1;31m# Check that this conversion is lossless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                     \u001b[0mconverted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                     \u001b[1;31m# We use the `values_eq` static function from TensorType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[1;31m# to handle NaN values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/misc/safe_asarray.pyc\u001b[0m in \u001b[0;36m_asarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert into dtype object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Note that dtype comparison must be done by comparing their `num`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# attribute. One cannot assume that two identical data types are pointers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('data_rem_road.pkl', 'rb') as f:\n",
    "    X_train, Seg_train, y_train, X_val, Seg_val, y_val, X_test, Seg_test, y_test = pickle.load(f)\n",
    "    \n",
    "with open('features_rem_road.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)\n",
    "    \n",
    "bins = [np.percentile(y_train, 0),\n",
    "        np.percentile(y_train, 25),\n",
    "        np.percentile(y_train, 50),\n",
    "        np.percentile(y_train, 75),\n",
    "        np.percentile(y_train, 100) + 1]\n",
    "\n",
    "y_train = np.digitize(y_train, bins) - 1\n",
    "y_val = np.digitize(y_val, bins) - 1\n",
    "y_test = np.digitize(y_test, bins) - 1\n",
    "\n",
    "input_layer = InputLayer((None, f1_train[0].shape[0]))\n",
    "ll1 = DenseLayer(input_layer, num_units=2048, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr1 = DropoutLayer(ll1, p=0.5)\n",
    "ll2 = DenseLayer(dr1, num_units=2048, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr3 = DropoutLayer(ll2, p=0.5)\n",
    "ll3 = DenseLayer(dr3, num_units=1024, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr4 = DropoutLayer(ll3, p=0.5)\n",
    "ll4 = DenseLayer(dr4, num_units=512, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr5 = DropoutLayer(ll4, p=0.5)\n",
    "ll5 = DenseLayer(dr5, num_units=256, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr6 = DropoutLayer(ll5, p=0.5)\n",
    "percentile = DenseLayer(dr6, num_units=len(set(y_train)), nonlinearity=softmax)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\", dtype='int32')\n",
    "input_X = T.matrix('X')\n",
    "\n",
    "probs = lasagne.layers.get_output(percentile, input_X, deterministic = True)\n",
    "y_predicted = np.argmax(probs, axis=1)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(percentile)\n",
    "print(all_weights)\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(probs, target_y).mean()\n",
    "l2_penalty = regularize_layer_params(percentile, l2) * 1e-1\n",
    "l1_penalty = regularize_layer_params(percentile, l1) * 1e-3\n",
    "loss += l2_penalty + l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(probs, target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights, learning_rate=0.01)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates = updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X, target_y],accuracy)\n",
    "\n",
    "num_epochs = 30 #amount of passes through the data\n",
    "\n",
    "batch_size = 100 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(f1_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fun(inputs, targets.astype(np.int32))\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(f1_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets.astype(np.int32))\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with road, 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_basic.pkl', 'rb') as f:\n",
    "    X_train, Seg_train, y_train, X_val, Seg_val, y_val, X_test, Seg_test, y_test = pickle.load(f)\n",
    "    \n",
    "with open('features_basic.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)\n",
    "    \n",
    "bins = [np.percentile(y_train, 0),\n",
    "        np.percentile(y_train, 25),\n",
    "        np.percentile(y_train, 50),\n",
    "        np.percentile(y_train, 75),\n",
    "        np.percentile(y_train, 100) + 1]\n",
    "\n",
    "y_train = np.digitize(y_train, bins) - 1\n",
    "y_val = np.digitize(y_val, bins) - 1\n",
    "y_test = np.digitize(y_test, bins) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b, W, b, W, b]\n",
      "Epoch 1 of 300 took 2.036s\n",
      "  training loss (in-iteration):\t\t140.387504\n",
      "  train accuracy:\t\t31.28 %\n",
      "  validation accuracy:\t\t52.80 %\n",
      "Epoch 2 of 300 took 2.016s\n",
      "  training loss (in-iteration):\t\t13.295092\n",
      "  train accuracy:\t\t55.17 %\n",
      "  validation accuracy:\t\t58.20 %\n",
      "Epoch 3 of 300 took 2.002s\n",
      "  training loss (in-iteration):\t\t12.705572\n",
      "  train accuracy:\t\t62.45 %\n",
      "  validation accuracy:\t\t60.40 %\n",
      "Epoch 4 of 300 took 2.003s\n",
      "  training loss (in-iteration):\t\t12.226186\n",
      "  train accuracy:\t\t68.86 %\n",
      "  validation accuracy:\t\t60.40 %\n",
      "Epoch 5 of 300 took 2.002s\n",
      "  training loss (in-iteration):\t\t11.880651\n",
      "  train accuracy:\t\t71.07 %\n",
      "  validation accuracy:\t\t60.20 %\n",
      "Epoch 6 of 300 took 2.014s\n",
      "  training loss (in-iteration):\t\t11.404465\n",
      "  train accuracy:\t\t79.07 %\n",
      "  validation accuracy:\t\t61.80 %\n",
      "Epoch 7 of 300 took 2.018s\n",
      "  training loss (in-iteration):\t\t11.053822\n",
      "  train accuracy:\t\t84.86 %\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 8 of 300 took 1.896s\n",
      "  training loss (in-iteration):\t\t10.977491\n",
      "  train accuracy:\t\t79.59 %\n",
      "  validation accuracy:\t\t61.00 %\n",
      "Epoch 9 of 300 took 2.035s\n",
      "  training loss (in-iteration):\t\t10.554585\n",
      "  train accuracy:\t\t87.14 %\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 10 of 300 took 1.963s\n",
      "  training loss (in-iteration):\t\t10.316450\n",
      "  train accuracy:\t\t90.83 %\n",
      "  validation accuracy:\t\t64.80 %\n",
      "Epoch 11 of 300 took 1.936s\n",
      "  training loss (in-iteration):\t\t10.013761\n",
      "  train accuracy:\t\t95.55 %\n",
      "  validation accuracy:\t\t63.40 %\n",
      "Epoch 12 of 300 took 1.933s\n",
      "  training loss (in-iteration):\t\t9.810423\n",
      "  train accuracy:\t\t97.24 %\n",
      "  validation accuracy:\t\t63.80 %\n",
      "Epoch 13 of 300 took 1.933s\n",
      "  training loss (in-iteration):\t\t9.722251\n",
      "  train accuracy:\t\t95.52 %\n",
      "  validation accuracy:\t\t63.00 %\n",
      "Epoch 14 of 300 took 1.922s\n",
      "  training loss (in-iteration):\t\t9.467655\n",
      "  train accuracy:\t\t99.24 %\n",
      "  validation accuracy:\t\t64.40 %\n",
      "Epoch 15 of 300 took 1.951s\n",
      "  training loss (in-iteration):\t\t9.321485\n",
      "  train accuracy:\t\t99.38 %\n",
      "  validation accuracy:\t\t66.20 %\n",
      "Epoch 16 of 300 took 1.924s\n",
      "  training loss (in-iteration):\t\t9.184269\n",
      "  train accuracy:\t\t99.72 %\n",
      "  validation accuracy:\t\t65.60 %\n",
      "Epoch 17 of 300 took 1.955s\n",
      "  training loss (in-iteration):\t\t9.060074\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t65.60 %\n",
      "Epoch 18 of 300 took 2.053s\n",
      "  training loss (in-iteration):\t\t8.943007\n",
      "  train accuracy:\t\t99.93 %\n",
      "  validation accuracy:\t\t64.40 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c68afadbdafd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer = InputLayer((None, f1_train[0].shape[0]))\n",
    "ll1 = DenseLayer(input_layer, num_units=2048, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr1 = DropoutLayer(ll1, p=0.5)\n",
    "ll2 = DenseLayer(dr1, num_units=2048, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr3 = DropoutLayer(ll2, p=0.5)\n",
    "ll3 = DenseLayer(dr3, num_units=1024, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr4 = DropoutLayer(ll3, p=0.5)\n",
    "ll4 = DenseLayer(dr4, num_units=512, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr5 = DropoutLayer(ll4, p=0.5)\n",
    "ll5 = DenseLayer(dr5, num_units=256, nonlinearity=lasagne.nonlinearities.elu)\n",
    "dr6 = DropoutLayer(ll5, p=0.5)\n",
    "percentile = DenseLayer(dr6, num_units=len(set(y_train)), nonlinearity=softmax)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\", dtype='int32')\n",
    "input_X = T.matrix('X')\n",
    "\n",
    "probs = lasagne.layers.get_output(percentile, input_X, deterministic = True)\n",
    "y_predicted = np.argmax(probs, axis=1)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(percentile)\n",
    "print(all_weights)\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(probs, target_y).mean()\n",
    "l2_penalty = regularize_layer_params(percentile, l2) * 1e-1\n",
    "l1_penalty = regularize_layer_params(percentile, l1) * 1e-2\n",
    "loss += l2_penalty + l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(probs, target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights, learning_rate=0.005)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates = updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X, target_y],accuracy)\n",
    "\n",
    "num_epochs = 300 #amount of passes through the data\n",
    "\n",
    "batch_size = 100 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(f1_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fun(inputs, targets.astype(np.int32))\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(f1_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets.astype(np.int32))\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network deleted road, 5 classes, stacked with segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 603979776 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 + (i0 * sgn(i1)))}}[(0, 0)].0)\nToposort index: 54\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(36864, 100), (100, 4096)]\nInputs strides: [(1, 36864), (4096, 1)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Sqr}[(0, 0)](GpuDot22.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7a80a87eb928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/user/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    913\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/user/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/user/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Error allocating 603979776 bytes of device memory (out of memory).\nApply node that caused the error: GpuDot22(GpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 + (i0 * sgn(i1)))}}[(0, 0)].0)\nToposort index: 54\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(36864, 100), (100, 4096)]\nInputs strides: [(1, 36864), (4096, 1)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Sqr}[(0, 0)](GpuDot22.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "with open('data_rem_road.pkl', 'rb') as f:\n",
    "    X_train, Seg_train, y_train, X_val, Seg_val, y_val, X_test, Seg_test, y_test = pickle.load(f)\n",
    "    \n",
    "with open('features_rem_road.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)\n",
    "    \n",
    "f1_train = np.hstack((np.array(f1_train), np.array(f2_train)))\n",
    "f1_val = np.hstack((np.array(f1_val), np.array(f2_val)))\n",
    "    \n",
    "bins = [np.percentile(y_train, 0),\n",
    "        np.percentile(y_train, 25),\n",
    "        np.percentile(y_train, 50),\n",
    "        np.percentile(y_train, 75),\n",
    "        np.percentile(y_train, 100) + 1]\n",
    "\n",
    "y_train = np.digitize(y_train, bins) - 1\n",
    "y_val = np.digitize(y_val, bins) - 1\n",
    "y_test = np.digitize(y_test, bins) - 1\n",
    "\n",
    "input_layer = InputLayer((None, f1_train[0].shape[0]))\n",
    "ll1 = DenseLayer(input_layer, num_units=4096)\n",
    "dr1 = DropoutLayer(ll1, p=0.5)\n",
    "ll2 = DenseLayer(dr1, num_units=4096)\n",
    "dr3 = DropoutLayer(ll2, p=0.5)\n",
    "percentile = DenseLayer(dr3, num_units=len(set(y_train)), nonlinearity=softmax)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\", dtype='int32')\n",
    "input_X = T.matrix('X')\n",
    "\n",
    "probs = lasagne.layers.get_output(percentile, input_X, deterministic = True)\n",
    "y_predicted = np.argmax(probs, axis=1)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(percentile)\n",
    "print(all_weights)\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(probs, target_y).mean()\n",
    "l2_penalty = regularize_layer_params(percentile, l2) * 1e-2\n",
    "l1_penalty = regularize_layer_params(percentile, l1) * 1e-3\n",
    "loss += l2_penalty + l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(probs, target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights, learning_rate=0.01)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates = updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X, target_y],accuracy)\n",
    "\n",
    "num_epochs = 30 #amount of passes through the data\n",
    "\n",
    "batch_size = 100 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(f1_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fun(inputs, targets.astype(np.int32))\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(f1_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets.astype(np.int32))\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with road, 5 classes, stacked with segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b]\n",
      "Epoch 1 of 30 took 4.315s\n",
      "  training loss (in-iteration):\t\t4726.092065\n",
      "  train accuracy:\t\t24.38 %\n",
      "  validation accuracy:\t\t33.00 %\n",
      "Epoch 2 of 30 took 4.367s\n",
      "  training loss (in-iteration):\t\t11.885494\n",
      "  train accuracy:\t\t30.03 %\n",
      "  validation accuracy:\t\t26.00 %\n",
      "Epoch 3 of 30 took 4.157s\n",
      "  training loss (in-iteration):\t\t5.118199\n",
      "  train accuracy:\t\t30.03 %\n",
      "  validation accuracy:\t\t25.20 %\n",
      "Epoch 4 of 30 took 4.156s\n",
      "  training loss (in-iteration):\t\t4.719048\n",
      "  train accuracy:\t\t33.24 %\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 5 of 30 took 4.154s\n",
      "  training loss (in-iteration):\t\t4.602650\n",
      "  train accuracy:\t\t33.83 %\n",
      "  validation accuracy:\t\t25.40 %\n",
      "Epoch 6 of 30 took 4.158s\n",
      "  training loss (in-iteration):\t\t4.511264\n",
      "  train accuracy:\t\t34.45 %\n",
      "  validation accuracy:\t\t29.20 %\n",
      "Epoch 7 of 30 took 4.156s\n",
      "  training loss (in-iteration):\t\t4.433514\n",
      "  train accuracy:\t\t35.31 %\n",
      "  validation accuracy:\t\t31.80 %\n",
      "Epoch 8 of 30 took 4.153s\n",
      "  training loss (in-iteration):\t\t4.364174\n",
      "  train accuracy:\t\t35.90 %\n",
      "  validation accuracy:\t\t33.00 %\n",
      "Epoch 9 of 30 took 4.321s\n",
      "  training loss (in-iteration):\t\t4.301553\n",
      "  train accuracy:\t\t36.66 %\n",
      "  validation accuracy:\t\t34.60 %\n",
      "Epoch 10 of 30 took 4.163s\n",
      "  training loss (in-iteration):\t\t4.242831\n",
      "  train accuracy:\t\t38.03 %\n",
      "  validation accuracy:\t\t36.00 %\n",
      "Epoch 11 of 30 took 4.158s\n",
      "  training loss (in-iteration):\t\t4.186211\n",
      "  train accuracy:\t\t40.17 %\n",
      "  validation accuracy:\t\t37.40 %\n",
      "Epoch 12 of 30 took 4.189s\n",
      "  training loss (in-iteration):\t\t4.131390\n",
      "  train accuracy:\t\t42.07 %\n",
      "  validation accuracy:\t\t37.60 %\n",
      "Epoch 13 of 30 took 4.163s\n",
      "  training loss (in-iteration):\t\t4.075547\n",
      "  train accuracy:\t\t43.79 %\n",
      "  validation accuracy:\t\t39.00 %\n",
      "Epoch 14 of 30 took 4.183s\n",
      "  training loss (in-iteration):\t\t4.020848\n",
      "  train accuracy:\t\t45.31 %\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 15 of 30 took 4.165s\n",
      "  training loss (in-iteration):\t\t3.965067\n",
      "  train accuracy:\t\t47.62 %\n",
      "  validation accuracy:\t\t42.20 %\n",
      "Epoch 16 of 30 took 4.164s\n",
      "  training loss (in-iteration):\t\t3.908799\n",
      "  train accuracy:\t\t50.52 %\n",
      "  validation accuracy:\t\t42.60 %\n",
      "Epoch 17 of 30 took 4.170s\n",
      "  training loss (in-iteration):\t\t3.857314\n",
      "  train accuracy:\t\t53.24 %\n",
      "  validation accuracy:\t\t43.60 %\n",
      "Epoch 18 of 30 took 4.167s\n",
      "  training loss (in-iteration):\t\t3.806450\n",
      "  train accuracy:\t\t55.59 %\n",
      "  validation accuracy:\t\t43.20 %\n",
      "Epoch 19 of 30 took 4.179s\n",
      "  training loss (in-iteration):\t\t3.757564\n",
      "  train accuracy:\t\t57.03 %\n",
      "  validation accuracy:\t\t43.20 %\n",
      "Epoch 20 of 30 took 4.165s\n",
      "  training loss (in-iteration):\t\t3.713492\n",
      "  train accuracy:\t\t58.41 %\n",
      "  validation accuracy:\t\t42.40 %\n",
      "Epoch 21 of 30 took 4.181s\n",
      "  training loss (in-iteration):\t\t3.673273\n",
      "  train accuracy:\t\t59.66 %\n",
      "  validation accuracy:\t\t42.20 %\n",
      "Epoch 22 of 30 took 4.169s\n",
      "  training loss (in-iteration):\t\t3.635788\n",
      "  train accuracy:\t\t60.59 %\n",
      "  validation accuracy:\t\t43.00 %\n",
      "Epoch 23 of 30 took 4.188s\n",
      "  training loss (in-iteration):\t\t3.601693\n",
      "  train accuracy:\t\t61.66 %\n",
      "  validation accuracy:\t\t40.60 %\n",
      "Epoch 24 of 30 took 4.169s\n",
      "  training loss (in-iteration):\t\t3.569888\n",
      "  train accuracy:\t\t63.17 %\n",
      "  validation accuracy:\t\t41.60 %\n",
      "Epoch 25 of 30 took 4.192s\n",
      "  training loss (in-iteration):\t\t3.539712\n",
      "  train accuracy:\t\t63.79 %\n",
      "  validation accuracy:\t\t41.00 %\n",
      "Epoch 26 of 30 took 4.172s\n",
      "  training loss (in-iteration):\t\t3.510056\n",
      "  train accuracy:\t\t64.86 %\n",
      "  validation accuracy:\t\t41.40 %\n",
      "Epoch 27 of 30 took 4.164s\n",
      "  training loss (in-iteration):\t\t3.482480\n",
      "  train accuracy:\t\t66.41 %\n",
      "  validation accuracy:\t\t40.20 %\n",
      "Epoch 28 of 30 took 4.187s\n",
      "  training loss (in-iteration):\t\t3.455721\n",
      "  train accuracy:\t\t67.69 %\n",
      "  validation accuracy:\t\t40.80 %\n",
      "Epoch 29 of 30 took 4.164s\n",
      "  training loss (in-iteration):\t\t3.428972\n",
      "  train accuracy:\t\t68.83 %\n",
      "  validation accuracy:\t\t41.20 %\n",
      "Epoch 30 of 30 took 4.173s\n",
      "  training loss (in-iteration):\t\t3.419649\n",
      "  train accuracy:\t\t68.41 %\n",
      "  validation accuracy:\t\t41.20 %\n"
     ]
    }
   ],
   "source": [
    "with open('data_basic.pkl', 'rb') as f:\n",
    "    X_train, Seg_train, y_train, X_val, Seg_val, y_val, X_test, Seg_test, y_test = pickle.load(f)\n",
    "    \n",
    "with open('features_basic.pkl', 'rb') as f:\n",
    "    f1_train, f1_val, f1_test, f2_train, f2_val, f2_test = pickle.load(f)\n",
    "    \n",
    "f1_train = np.hstack((np.array(f1_train), np.array(f2_train)))\n",
    "f1_val = np.hstack((np.array(f1_val), np.array(f2_val)))\n",
    "    \n",
    "bins = [np.percentile(y_train, 0),\n",
    "        np.percentile(y_train, 25),\n",
    "        np.percentile(y_train, 50),\n",
    "        np.percentile(y_train, 75),\n",
    "        np.percentile(y_train, 100) + 1]\n",
    "\n",
    "y_train = np.digitize(y_train, bins) - 1\n",
    "y_val = np.digitize(y_val, bins) - 1\n",
    "y_test = np.digitize(y_test, bins) - 1\n",
    "\n",
    "input_layer = InputLayer((None, f1_train[0].shape[0]))\n",
    "ll1 = DenseLayer(input_layer, num_units=4096)\n",
    "dr1 = DropoutLayer(ll1, p=0.5)\n",
    "ll2 = DenseLayer(dr1, num_units=4096)\n",
    "dr3 = DropoutLayer(ll2, p=0.5)\n",
    "percentile = DenseLayer(dr3, num_units=len(set(y_train)), nonlinearity=softmax)\n",
    "\n",
    "target_y = T.vector(\"target Y integer\", dtype='int32')\n",
    "input_X = T.matrix('X')\n",
    "\n",
    "probs = lasagne.layers.get_output(percentile, input_X, deterministic = True)\n",
    "y_predicted = np.argmax(probs, axis=1)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(percentile)\n",
    "print(all_weights)\n",
    "\n",
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(probs, target_y).mean()\n",
    "l2_penalty = regularize_layer_params(percentile, l2) * 1e-1\n",
    "l1_penalty = regularize_layer_params(percentile, l1) * 1e-2\n",
    "loss += l2_penalty + l1_penalty\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(probs, target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.adagrad(loss, all_weights, learning_rate=0.01)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates = updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X, target_y],accuracy)\n",
    "\n",
    "num_epochs = 30 #amount of passes through the data\n",
    "\n",
    "batch_size = 100 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(f1_train, y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fun(inputs, targets.astype(np.int32))\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(f1_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets.astype(np.int32))\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
